{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables reside within a database. By default, Spark creates tables under the default database. To create your own database name, you can issue a SQL command from your Spark application or notebook. Using the US flight delays data set, let’s create both a *managed* and an *unmanaged* table. To begin, we’ll create a database called `learn_spark_db` and tell Spark we want to use that database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/zacks/Git/Data%20Science%20Projects/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 20:17:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"SparkSQLHiveExample\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to data set\n",
    "csv_file = \"../../../databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 20:17:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/03/01 20:17:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/03/01 20:17:49 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/03/01 20:17:49 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore zacks@192.168.86.250\n",
      "22/03/01 20:17:49 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "22/03/01 20:17:50 WARN ObjectStore: Failed to get database learn_spark_db, returning NoSuchObjectException\n",
      "22/03/01 20:17:50 WARN ObjectStore: Failed to get database learn_spark_db, returning NoSuchObjectException\n",
      "22/03/01 20:17:50 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/03/01 20:17:50 WARN ObjectStore: Failed to get database learn_spark_db, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop database and it's tables\n",
    "spark.sql(\"drop database if exists learn_spark_db cascade\")\n",
    "# create database and use database\n",
    "spark.sql(\"create database learn_spark_db\")\n",
    "spark.sql(\"use learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, any commands we issue in our application to create tables will result in the tables being created in this database and residing under the database name `learn_spark_db`.\n",
    "\n",
    "### Creating a managed table\n",
    "\n",
    "To create a managed table within the database learn_spark_db, you can issue a SQL query like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 20:17:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "22/03/01 20:17:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "22/03/01 20:17:50 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "22/03/01 20:17:50 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/03/01 20:17:50 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/03/01 20:17:50 WARN HiveMetaStore: Location: file:/Users/zacks/Git/Data Science Projects/LearningSparkV2/chapter4/py/src/spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl specified for non-external table:managed_us_delay_flights_tbl\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlQuery = \"\"\"\n",
    "create table if not exists managed_us_delay_flights_tbl as\n",
    "select * from us_delay_flights_tbl\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1121845|   -9|     198|   FSM|        DFW|\n",
      "|1130620|   -8|     198|   FSM|        DFW|\n",
      "|1131005|    0|     198|   FSM|        DFW|\n",
      "|1131430|    0|     198|   FSM|        DFW|\n",
      "|1131845|    3|     198|   FSM|        DFW|\n",
      "|1140620|   -6|     198|   FSM|        DFW|\n",
      "|1141005|   -7|     198|   FSM|        DFW|\n",
      "|1141430|  -10|     198|   FSM|        DFW|\n",
      "|1141845|  -12|     198|   FSM|        DFW|\n",
      "|1150620|  -10|     198|   FSM|        DFW|\n",
      "|1151005|    1|     198|   FSM|        DFW|\n",
      "|1151430|   -1|     198|   FSM|        DFW|\n",
      "|1151845|   -7|     198|   FSM|        DFW|\n",
      "|1160620|   -5|     198|   FSM|        DFW|\n",
      "|1161005|   -5|     198|   FSM|        DFW|\n",
      "|1161430|   -4|     198|   FSM|        DFW|\n",
      "|1161845|   -6|     198|   FSM|        DFW|\n",
      "|1170620|   -1|     198|   FSM|        DFW|\n",
      "|1171005|   -7|     198|   FSM|        DFW|\n",
      "|1171430|   18|     198|   FSM|        DFW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from managed_us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# drop managed_us_delay_flights_tbl we create through Spark SQL API\n",
    "spark.sql(\"drop table if exists managed_us_delay_flights_tbl\")\n",
    "# Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an unmanaged table\n",
    "\n",
    "By contrast, you can create unmanaged tables from your own data sources—say, Parquet, CSV, or JSON files stored in a file store accessible to your Spark application.\n",
    "\n",
    "To create an unmanaged table from a data source such as a CSV file, in SQL use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 20:17:56 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `learn_spark_db`.`us_delay_flights_tbl` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlQuery = f\"\"\"\n",
    "CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING)\n",
    "USING csv OPTIONS (PATH \"{csv_file}\")\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And within the DataFrame API use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 20:18:20 WARN HadoopFSUtils: The directory file:/Users/zacks/Git/Data Science Projects/LearningSparkV2/chapter4/py/databricks-datasets/learning-spark-v2/flights/departuredelays.csv was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop table we created in previous steps\n",
    "spark.sql(\"drop table if exists us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# you may also need to remove /tmp/data/us_flights_delay at first\n",
    "# rm -rf /tmp/data\n",
    "(flights_df\n",
    "    .write\n",
    "    .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    "    .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01201755|    0|     449|   ORF|        ATL|\n",
      "|01201610|   52|     449|   ORF|        ATL|\n",
      "|01201441|    0|     449|   ORF|        ATL|\n",
      "|01211755|  -15|     449|   ORF|        ATL|\n",
      "|01210941|   -5|     449|   ORF|        ATL|\n",
      "|01210700|    5|     449|   ORF|        ATL|\n",
      "|01211243|    2|     449|   ORF|        ATL|\n",
      "|01210540|   -5|     449|   ORF|        ATL|\n",
      "|01211610|   27|     449|   ORF|        ATL|\n",
      "|01211441|   -6|     449|   ORF|        ATL|\n",
      "|01221755|   -4|     449|   ORF|        ATL|\n",
      "|01220941|   -5|     449|   ORF|        ATL|\n",
      "|01220700|   -2|     449|   ORF|        ATL|\n",
      "|01221243|    0|     449|   ORF|        ATL|\n",
      "|01220540|   -5|     449|   ORF|        ATL|\n",
      "|01221610|   -5|     449|   ORF|        ATL|\n",
      "|01221441|   -2|     449|   ORF|        ATL|\n",
      "|01231755|   -1|     449|   ORF|        ATL|\n",
      "|01230941|   -8|     449|   ORF|        ATL|\n",
      "|01230700|   -3|     449|   ORF|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Views\n",
    "\n",
    "In addition to creating tables, Spark can create views on top of existing tables. Views can be global (visible across all `SparkSessions` on a given cluster) or session-scoped (visible only to a single `SparkSession`), and they are temporary: they disappear after your Spark application terminates.\n",
    "\n",
    "[Creating views](https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-view.html#id1) has a similar syntax to creating tables within a database. Once you cre‐ ate a view, you can query it as you would a table. The difference between a view and a table is that views don’t actually hold the data; tables persist after your Spark applica‐ tion terminates, but views disappear.\n",
    "\n",
    "You can create a view from an existing table using SQL. For example, if you wish to work on only the subset of the US flight delays data set with origin airports of New York (JFK) and San Francisco (SFO), the following queries will create global temporary and temporary views consisting of just that slice of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlQuery = \"\"\"\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "SELECT date, delay, origin, destination\n",
    "from us_delay_flights_tbl\n",
    "WHERE origin = 'SFO';\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery)\n",
    "\n",
    "sqlQuery = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "SELECT date, delay, origin, destination\n",
    "from us_delay_flights_tbl\n",
    "WHERE origin = 'JFK'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve created these views, you can issue queries against them just as you would against a table. Keep in mind that when accessing a global temporary view you must use the prefix `global_temp.<view_name>`, because Spark creates global temporary views in a global temporary database called `global_temp`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query global view from global_temp\n",
    "spark.sql(\"select * from global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, you can access the normal temporary view without the `global_temp` prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02010900|   -1|   JFK|        LAX|\n",
      "|02011200|   -5|   JFK|        LAX|\n",
      "|02011030|   -6|   JFK|        LAX|\n",
      "|02011900|   -1|   JFK|        LAX|\n",
      "|02011700|   -3|   JFK|        LAS|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query temporary view from current database\n",
    "spark.sql(\"select * from us_origin_airport_JFK_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can accomplish the same thing with the DataFrame API as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also query table from DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query global view from global_temp\n",
    "spark.read.table(\"global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02010900|   -1|   JFK|        LAX|\n",
      "|02011200|   -5|   JFK|        LAX|\n",
      "|02011030|   -6|   JFK|        LAX|\n",
      "|02011900|   -1|   JFK|        LAX|\n",
      "|02011700|   -3|   JFK|        LAS|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query temporary view from current database\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------------------+-----------+\n",
      "|namespace  |viewName                             |isTemporary|\n",
      "+-----------+-------------------------------------+-----------+\n",
      "|global_temp|us_origin_airport_sfo_global_tmp_view|true       |\n",
      "|           |us_origin_airport_jfk_tmp_view       |true       |\n",
      "+-----------+-------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all views in global temp view database and current database\n",
    "spark.sql(\"show views in global_temp;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_origin_airport_sfo_global_tmp_view', database='global_temp', description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_origin_airport_jfk_tmp_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or\n",
    "# List all views in global temp view database and current database\n",
    "spark.catalog.listTables(dbName=\"global_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop views\n",
    "spark.sql(\"drop view if exists us_origin_airport_sfo_global_tmp_view\")\n",
    "spark.sql(\"drop view if exists us_origin_airport_jfk_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or\n",
    "# drop views\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_sfo_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_jfk_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d877e96881ad52e70a32ec53716a689c80db81b01fdbf6505bd55c9db331f8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('LearningSparkV2-IJjgJkTy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
