{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/zacks/Git/Data%20Science%20Projects/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 11:58:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"SparkSQLExampleApp\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Path to data set\n",
    "csv_file = \"../../../databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a temporary view, we can issue SQL queries using Spark SQL. These queries are no different from those you might issue against a SQL table in, say, a MySQL or PostgreSQL database. The point here is to show that Spark SQL offers an ANSI:2003–compliant SQL interface, and to demonstrate the interoperability between SQL and DataFrames.\n",
    "\n",
    "The US flight delays data set has five columns:\n",
    "\n",
    "- The date column contains a string like 02190925. When converted, this maps to 02-19 09:25 am.\n",
    "- The delay column gives the delay in minutes between the scheduled and actual departure times. Early departures show negative numbers.\n",
    "- The distance column gives the distance in miles from the origin airport to the destination airport.\n",
    "- The origin column contains the origin IATA airport code.\n",
    "- The destination column contains the destination IATA airport code.\n",
    "\n",
    "With that in mind, let’s try some example queries against this data set. First, we’ll find all flights whose distance is greater than 1,000 miles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqlQuery = \"\"\"\n",
    "select distance, origin, destination\n",
    "from us_delay_flights_tbl\n",
    "where distance > 1000\n",
    "order by distance desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the results show, all of the longest flights were between Honolulu (HNL) and New York (JFK). Next, we’ll find all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlQuery = \"\"\"\n",
    "select date, delay, origin, destination\n",
    "from us_delay_flights_tbl\n",
    "where delay > 120\n",
    "    and origin = 'SFO'\n",
    "    and destination = 'ORD'\n",
    "order by delay desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there were many significantly delayed flights between these two cities, on different dates. (As an exercise, convert the `date` column into a readable format and find the days or months when these delays were most common. Were the delays related to winter months or holidays?)\n",
    "\n",
    "Let’s try a more complicated query where we use the `CASE` clause in SQL. In the following example, we want to label all US flights, regardless of origin and destination, with an indication of the delays they experienced: Very Long Delays (> 6 hours), Long Delays (2–6 hours), etc. We’ll add these human-readable labels in a new column called `Flight_Delays`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL API\n",
    "sqlQuery = \"\"\"\n",
    "select\n",
    "    delay, origin, destination,\n",
    "    case\n",
    "        when delay > 360 then 'Very Long Delays'\n",
    "        when delay > 120 and delay <= 360 then 'Long Delays'\n",
    "        when delay > 60 and delay <= 120 then 'Short Delays'\n",
    "        when delay > 0 and delay <= 60 then 'Tolerable Delays'\n",
    "        when delay = 0 then 'No Delays'\n",
    "        else 'Early'\n",
    "    end as Flight_Delays\n",
    "from us_delay_flights_tbl\n",
    "order by origin, delay desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|   Flight_Delays|counts|\n",
      "+----------------+------+\n",
      "|           Early|668729|\n",
      "|Tolerable Delays|497742|\n",
      "|       No Delays|131122|\n",
      "|    Short Delays| 61039|\n",
      "|     Long Delays| 31447|\n",
      "|Very Long Delays|  1499|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL API\n",
    "sqlQuery = \"\"\"\n",
    "with cte as\n",
    "(\n",
    "    select\n",
    "        delay, origin, destination,\n",
    "        case\n",
    "            when delay > 360 then 'Very Long Delays'\n",
    "            when delay > 120 and delay <= 360 then 'Long Delays'\n",
    "            when delay > 60 and delay <= 120 then 'Short Delays'\n",
    "            when delay > 0 and delay <= 60 then 'Tolerable Delays'\n",
    "            when delay = 0 then 'No Delays'\n",
    "            else 'Early'\n",
    "        end as Flight_Delays\n",
    "    from us_delay_flights_tbl\n",
    "    order by origin, delay desc\n",
    ")\n",
    "select Flight_Delays, count(*) as counts\n",
    "from cte\n",
    "group by Flight_Delays\n",
    "order by 2 desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(\"distance > 1000\")\n",
    "    .orderBy(\"distance\", ascending=False)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, asc, desc\n",
    "\n",
    "# Spark SQL API\n",
    "case = \"\"\"\n",
    "case\n",
    "    when delay > 360 then 'Very Long Delays'\n",
    "    when delay > 120 and delay <= 360 then 'Long Delays'\n",
    "    when delay > 60 and delay <= 120 then 'Short Delays'\n",
    "    when delay > 0 and delay <= 60 then 'Tolerable Delays'\n",
    "    when delay = 0 then 'No Delays'\n",
    "    else 'Early'\n",
    "end as Flight_Delays\n",
    "\"\"\"\n",
    "\n",
    "(df.select(\"delay\", \"origin\", \"destination\", expr(case))\n",
    "    .orderBy(asc(\"origin\"), desc(\"delay\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.select(\"delay\", \"origin\", \"destination\", expr(case))\n",
    "    .orderBy(col(\"origin\").asc(), col(\"delay\").desc())).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d877e96881ad52e70a32ec53716a689c80db81b01fdbf6505bd55c9db331f8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('LearningSparkV2-IJjgJkTy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
